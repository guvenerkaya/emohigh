{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import dlib\n",
    "import math\n",
    "import itertools\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"/Users/guvenerkaya/Downloads/dlib_shape_predictor\") \n",
    "\n",
    "data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_landmarks(image):\n",
    "    data_points = []\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #convert to grayscale\n",
    "    clahe_image = clahe.apply(gray) #Contrast Limited Adaptive Histogram Equalization\n",
    "    detections = detector(gray, 1)\n",
    "    if len(detections) > 0:\n",
    "        for k,d in enumerate(detections): #For all detected face instances individually\n",
    "            shape = predictor(image, d) #Draw facial landmarks with dlib's predictor class\n",
    "            xlist = []\n",
    "            ylist = []\n",
    "            for i in range(0,68): #Store X and Y coordinates of the facial landmarks in two lists\n",
    "                xlist.append(float(shape.part(i).x))\n",
    "                ylist.append(float(shape.part(i).y))\n",
    "            landmarks = []\n",
    "            for x, y in zip(xlist, ylist):\n",
    "                landmarks.append(x) # get them in order in the same list wit(x_1,y_1,x_2,y_2...)\n",
    "                landmarks.append(y)\n",
    "            data['landmarks'] = landmarks \n",
    "            data_points.append(data['landmarks'])\n",
    "        return data_points\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "key_landmarks = []\n",
    "no_face_detected = []\n",
    "detected_faces = []\n",
    "video_num = []\n",
    "keyframe_num =[]\n",
    "\n",
    "files = glob.glob('/Volumes/LIRIS_ACCEDE/keyframes 2/*.png') #read all extracted key frames\n",
    "for file in files:\n",
    "    \n",
    "#image = cv2.imread('/Volumes/LIRIS_ACCEDE/keyframes 2/ACCEDE00195_03.png') # read image/frame\n",
    "    image = cv2.imread(file) #store each key frame one by one to draw facial landmarks\n",
    "    data_points = get_landmarks(image) #store facial landmarks \n",
    "    if data_points != None: #if face is found\n",
    "        for i in range(len(data_points)): #adding the facial landmarks of each found face to the list key_landmarks\n",
    "            key_landmarks.append(data_points[i])\n",
    "            video_number = file[40:45] #detecting which video the key frames belong \n",
    "            video_num.append(video_number)\n",
    "            keyframe_number = file[46:48]#detecting the which keyframe of the video it is\n",
    "            keyframe_num.append(keyframe_number)\n",
    "            detected_faces.append(i+1)#detecting the number of detected faces in the key frame\n",
    "         \n",
    "    #else:\n",
    "      #  no_face_detected.append(file[34:])\n",
    "\n",
    "\n",
    "#df1 = pd.DataFrame(data_points, columns = name)\n",
    "#print(len(files))    \n",
    "#print(files)\n",
    "#set column names\n",
    "name_x = np.arange(1, 69, 1)\n",
    "name_x = list(map(str, name_x))\n",
    "name_x = [\"x_\" + item for item in name_x]\n",
    "\n",
    "name_y = np.arange(1, 69, 1)\n",
    "name_y = list(map(str, name_y))\n",
    "name_y = [\"y_\" + item for item in name_y]\n",
    "\n",
    "name = [None]*(len(name_x)+len(name_y))\n",
    "name[::2] = name_x\n",
    "name[1::2] = name_y\n",
    "\n",
    "df = pd.DataFrame(key_landmarks, index = video_num,   columns=name)\n",
    "df.set_index( pd.Index(keyframe_num), append=True, inplace=True)\n",
    "df.set_index(pd.Index(detected_faces), append = True, inplace= True)\n",
    "df.index.names = ['id','keyframe','face']\n",
    "df_original = df\n",
    "df\n",
    "#video_num\n",
    "#keyframe_num\n",
    "#pickle.dump(df, open('/Users/guvenerkaya/Desktop/datasets_and_models/kf_facial_landmarks', 'wb')) # save key frames with drawn facial landmarks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.values\n",
    "\n",
    "final_data = []\n",
    "#calculate the euclidean distances of facial landarks of the key frames\n",
    "for row in range(0, len(df)):\n",
    "    data = df[row]\n",
    "    #print(data)\n",
    "    print(\"\\n\")\n",
    "    x = []\n",
    "    y = []\n",
    "    new_data = []\n",
    "    x = data[::2]\n",
    "    y = data[1::2]\n",
    "    #print(x)\n",
    "    #print(\"\\n\")\n",
    "    #print(y)\n",
    "    #print(\"\\n\")\n",
    "    for i in range(0, 68):\n",
    "        for j in range(i+1, 68):\n",
    "            distance = sqrt((x[i] - x[j]) * (x[i] - x[j]) + (y[i] - y[j]) * (y[i] - y[j])) \n",
    "            new_data.append(distance)\n",
    "    final_data.append(new_data)\n",
    "#set column names\n",
    "names = []\n",
    "for i in range(1, 69, 1):\n",
    "    for j in range(i+1, 69, 1):\n",
    "        name = \"distance\" + str(i) + \"<->\" + str(j)\n",
    "        names.append(name)\n",
    "\n",
    "df_new = pd.DataFrame(final_data, columns=names, index=df_original.index)\n",
    "#extend the data frame with euclidean distances\n",
    "df3 = pd.concat([df_original, df_new], axis=1, sort=False)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#assess only the best features\n",
    "best = ['distance64<->66', 'distance25<->44', 'distance45<->47', 'distance65<->68', 'distance44<->48', 'distance20<->39', 'distance63<->67', 'distance34<->63', 'distance62<->68', 'distance60<->62', 'distance21<->39', 'distance21<->41', 'distance52<->67', 'distance54<->56', 'distance54<->68', 'distance21<->40', 'distance49<->65', 'distance53<->57', 'distance20<->42', 'distance10<->55', 'distance36<->57', 'distance53<->66', 'distance49<->58', 'distance24<->43', 'distance57<->64', 'distance9<->61', 'distance64<->67', 'distance50<->68', 'distance49<->59', 'distance25<->47', 'distance32<->34', 'distance31<->63', 'distance39<->42', 'distance49<->60', 'distance59<->62', 'distance60<->61', 'distance51<->53', 'distance35<->55', 'distance38<->42', 'distance49<->56', 'distance50<->57', 'distance63<->65', 'distance31<->62', 'distance21<->42', 'distance32<->61', 'distance20<->41', 'distance54<->61', 'distance58<->63', 'distance10<->65', 'distance9<->59']\n",
    "df4 = df3[best] \n",
    "df4 #saved as 'kf_50_best_distances'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SVM = pickle.load(open('/Users/guvenerkaya/Desktop/datasets_and_models/SVM_trained', 'rb'))#load trained svm \n",
    "test  = SVM.predict_proba(df4)  #find the probabilities  \n",
    "#test = SVM.predict(df5)\n",
    "a =pd.DataFrame(test)\n",
    "#a\n",
    "x = df4.index #setindex\n",
    "a.index = x\n",
    "SVM.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a.columns = ['angry', 'contemptuous', 'disgusted', 'fearful' ,'happy', 'neutral' ,'sad', 'surprised']#set columns according to SVM classes\n",
    "#by_row_index = a.groupby(a.index)\n",
    "#df_means = by_row_index.mean(level='id')\n",
    "df_means= a.mean(level='id')  #calculate the means of the emotion prabibilities with the same id\n",
    "index = df_means.index.to_list \n",
    "\n",
    "index_int = df_means.index.astype(str).astype(int) #change type of index to manipulate it\n",
    "df_means.index = index_int\n",
    "df_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#find_missing finds the missing values in a given list of numbers\n",
    "def find_missing(lst): \n",
    "    return [x for x in range(lst[0], lst[-1]+1)  \n",
    "                               if x not in lst] \n",
    "feature_list=['angry', 'contemptuous', 'disgusted' ,'fearful','happy', 'neutral', 'sad',\n",
    " 'surprised']\n",
    "zero_data = np.zeros(shape=(4534,len(feature_list)))\n",
    "feature_list=['angry', 'contemptuous', 'disgusted' ,'fearful','happy', 'neutral', 'sad',\n",
    " 'surprised']\n",
    "#make a zero dataframe for missing indices in key frame emotion probabilities (when no faces found in key frames)\n",
    "index_list = find_missing(index_int)\n",
    "df0 = pd.DataFrame(zero_data,index =index_list , columns=feature_list)\n",
    "df0\n",
    "last_df = df_means.append(df0,ignore_index=False)\n",
    "last_df.index.name = 'id'\n",
    "last_df\n",
    "\n",
    "df_sorted = last_df.sort_values('id', axis=0) #sort the dataframe depending the index values\n",
    "last_row = [0,0,0,0,0,0,0,0]\n",
    "add_last_row =  pd.DataFrame({'angry':0, 'contemptuous':0, 'disgusted':0, 'fearful':0, 'happy':0, 'neutral':0, 'sad':0, 'surprised':0}, index =[0])\n",
    "#last_of_us = pd.concat([add_last_row, df_sorted],)\n",
    "#pickle.dump(last_of_us, open('/Users/guvenerkaya/Desktop/datasets_and_models/emotion_proabilities', 'wb'))\n",
    "#index_list[-1]\n",
    "last_of_us = pd.concat([add_last_row,df_sorted],  ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set detailed index for emotion probabilities\n",
    "lastlast= last_of_us.set_index(index_dfr.index)\n",
    "index_dfr = pickle.load(open('/Users/guvenerkaya/Desktop/datasets_and_models/valence_dataframe','rb'))\n",
    "#pickle.dump(last_of_us, open('/Users/guvenerkaya/Desktop/datasets_and_models/emotion_probabilities', 'wb'))\n",
    "lastlast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#add emotion probabilities to arousal dataframe\n",
    "arousal_data = pd.read_pickle('/Users/guvenerkaya/Desktop/datasets_and_models/arousal_dataframe')\n",
    "arousal_features = arousal_data.iloc[:,:-1]\n",
    "arousal_new = arousal_data.reset_index(drop=True)\n",
    "arousal_new.iloc[-1]\n",
    "concencated = arousal_new.join(last_of_us)\n",
    "final= concencated.drop(labels='arousalValue', axis = 1)\n",
    "final.insert(31, 'arousalValue',arousal_new.iloc[:,-1])\n",
    "final\n",
    "#arousal dataframe with emotion probabilities are saved as 'arousal_with_emotionsprobabilities'\n",
    "#index_df = pickle.load(open('/Users/guvenerkaya/Desktop/datasets_and_models/valence_dataframe','rb'))\n",
    "finalfinal=final.set_index(index_df.index) # set index including the ids and videonames\n",
    "#pickle.dump(finalfinal,open('arousal_with_emotionsprobabilities','wb'))\n",
    "\n",
    "#add emotion probabilities to valence dataframe and save as 'valence_with_emotionsprobabilities'\n",
    "valence_data= pd.read_pickle('/Users/guvenerkaya/Desktop/datasets_and_models/valence_dataframe')\n",
    "valence_features = valence_data.iloc[:,:-1]\n",
    "concat = valence_features.join(lastlast)\n",
    "concat\n",
    "concat.insert(25,'valenceValue',valence_data.iloc[:,-1])\n",
    "concat\n",
    "pickle.dump(concat,open('/Users/guvenerkaya/Desktop/datasets_and_models/valence_with_emotionsprobabilities','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y= final.iloc[:,31]\n",
    "X=final.iloc[:,:31]\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.24, random_state=0)\n",
    "lr= LinearRegression()\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test,y_pred)\n",
    "import scipy\n",
    "scipy.stats.pearsonr(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "arousal_X = arousal_data.iloc[:,:23]\n",
    "arousal_y = arousal_data.iloc[:,23]\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(arousal_X, arousal_y, test_size=0.25, random_state=0)\n",
    "lr_1 = LinearRegression()\n",
    "lr_1.fit(X_train1,y_train1)\n",
    "y_pred1 = lr_1.predict(X_test1)\n",
    "r2_score(y_test1,y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
